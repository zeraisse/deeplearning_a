import torch
import glob
import os
import warnings
from transformers import AutoTokenizer

# On importe la classe du mod√®le et les params depuis ton fichier d'entra√Ænement
# Assure-toi que le fichier s'appelle bien 'train_qa.py'
try:
    from train_qa import TransformerModel, MAX_LEN
except ImportError:
    print("‚ùå Erreur : Impossible d'importer TransformerModel depuis train_qa.py")
    print("V√©rifie que tu es dans le bon dossier.")
    exit()

# --- CONFIGURATION ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
warnings.filterwarnings("ignore")

# --- 1. RECHERCHE AUTOMATIQUE DU DERNIER CHECKPOINT ---
# On cherche dans le dossier par d√©faut de Lightning
list_of_checkpoints = glob.glob('logs/squad_experiment/**/checkpoints/*.ckpt', recursive=True)

# Si vide, on cherche √† la racine ou dans le dossier checkpoints simple
if not list_of_checkpoints:
    list_of_checkpoints = glob.glob('checkpoints/*.ckpt')

if not list_of_checkpoints:
    print("‚ùå Aucun checkpoint trouv√© (ni dans logs/, ni dans checkpoints/).")
    print("Lance d'abord : python train_qa.py")
    exit()
else:
    # On prend le fichier le plus r√©cent
    MODEL_PATH = max(list_of_checkpoints, key=os.path.getctime)
    print(f"üìÇ Checkpoint trouv√© : {MODEL_PATH}")

# --- 2. CHARGEMENT ---
print("‚è≥ Chargement du Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

print(f"üèóÔ∏è  Chargement de l'architecture...")
model = TransformerModel().to(DEVICE)

print("‚öñÔ∏è  Chargement des poids...")
try:
    # --- CHARGEMENT LIGHTNING ---
    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
    
    # Si c'est un checkpoint Lightning, les cl√©s commencent par "model."
    # Si c'est un save manuel, non. On g√®re les deux cas.
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    new_state_dict = {}
    for key, value in state_dict.items():
        # On nettoie le pr√©fixe "model." ajout√© par Lightning
        if key.startswith("model."):
            new_key = key.replace("model.", "") 
            new_state_dict[new_key] = value
        else:
            new_state_dict[key] = value
            
    model.load_state_dict(new_state_dict)
    print("‚úÖ Poids charg√©s avec succ√®s !")
    
    model.eval()

except RuntimeError as e:
    print(f"‚ùå Erreur d'architecture : {e}")
    print("Conseil : V√©rifie que tes hyperparam√®tres (EMBED_DIM, LAYERS, etc.) dans train_qa.py sont les m√™mes qu'√† l'entra√Ænement.")
    exit()

# --- 3. G√âN√âRATION (Via Beam Search) ---
def generate_answer(question, context):
    # Pr√©paration
    input_text = f"{question} [SEP] {context}"
    enc_tokens = tokenizer(input_text, max_length=MAX_LEN, padding="max_length", truncation=True, return_tensors="pt")
    
    # Ajout de la dimension de batch (unsqueeze) car le mod√®le attend [Batch, Seq]
    src = enc_tokens['input_ids'].to(DEVICE)

    with torch.no_grad():
        # Appel direct √† la fonction beam_search de ton mod√®le
        # Elle g√®re d√©j√† le [CLS] de d√©part et la boucle
        generated_ids = model.beam_search(src, tokenizer, beam_width=3, max_gen_len=30)

    # D√©codage
    return tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)

# --- 4. INTERFACE ---
print("\n" + "="*50)
print(f"ü§ñ ORACLE SQuAD (G√©n√©ratif)")
print("="*50)

while True:
    print("\n--- üìù NOUVEAU CONTEXTE ---")
    context = input("Texte : ")
    if not context.strip(): continue
    if context.lower() in ['exit', 'quit', 'q']: break
    
    while True:
        question = input("\n‚ùì Question (ou 'new' pour changer de texte) : ")
        if question.lower() == 'new': break
        if question.lower() in ['exit', 'quit', 'q']: exit()
        
        try:
            print("ü§î R√©flexion...")
            reponse = generate_answer(question, context)
            print(f"üí° R√©ponse : \033[1m{reponse}\033[0m") # En gras
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration : {e}")